{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:365: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Paris is the capital of France.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0, convert_system_message_to_human=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can answer questions.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "message = HumanMessage(content=\"What is the capital of France?\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(message).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/7d0j8gd13s9b6p9p9qhrhz800000gn/T/ipykernel_7683/1742984577.py:21: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"previous_messages\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are having a chat with a user.\n",
      "    Previous messages: []\n",
      "    User: Hello, how are you?\n",
      "    Assistant:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'previous_messages': [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={})],\n",
       " 'input': 'Hello, how are you?',\n",
       " 'assistant': \"I'm doing well, thank you for asking! How are you today?\"}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True,\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"previous_messages\", \"input\"],\n",
    "    template=\"\"\"\n",
    "    You are having a chat with a user.\n",
    "    Previous messages: {previous_messages}\n",
    "    User: {input}\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"previous_messages\", return_messages=True)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"assistant\", memory=memory, verbose=True)\n",
    "\n",
    "chain.invoke({\"previous_messages\": \"\", \"input\": \"Hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are having a chat with a user.\n",
      "    Previous messages: [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={})]\n",
      "    User: I'm good, thank you!\n",
      "    Assistant:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'previous_messages': [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm good, thank you!\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great to hear!  Is there anything I can help you with today?\", additional_kwargs={}, response_metadata={})],\n",
       " 'input': \"I'm good, thank you!\",\n",
       " 'assistant': \"That's great to hear!  Is there anything I can help you with today?\"}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"previous_messages\": \"\", \"input\": \"I'm good, thank you!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are having a chat with a user.\n",
      "    Previous messages: [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm good, thank you!\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great to hear!  Is there anything I can help you with today?\", additional_kwargs={}, response_metadata={})]\n",
      "    User: What is the capital of France?\n",
      "    Assistant:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'previous_messages': [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm good, thank you!\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great to hear!  Is there anything I can help you with today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={})],\n",
       " 'input': 'What is the capital of France?',\n",
       " 'assistant': 'The capital of France is Paris.'}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"previous_messages\": \"\", \"input\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are having a chat with a user.\n",
      "    Previous messages: [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm good, thank you!\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great to hear!  Is there anything I can help you with today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={})]\n",
      "    User: i thought it was london\n",
      "    Assistant:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'previous_messages': [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm good, thank you!\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great to hear!  Is there anything I can help you with today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='i thought it was london', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='London is the capital of the United Kingdom, not France.  Paris is the capital of France.', additional_kwargs={}, response_metadata={})],\n",
       " 'input': 'i thought it was london',\n",
       " 'assistant': 'London is the capital of the United Kingdom, not France.  Paris is the capital of France.'}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"previous_messages\": \"\", \"input\": \"i thought it was london\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
